<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>Benjamin L Albritton</title>
        <description>Benjamin L Albritton - Benjamin Albritton</description>
        <link>http://blalbrit.github.io</link>
        <atom:link href="http://blalbrit.github.io/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Mon, 17 Jul 2017 16:03:43 -0700</lastBuildDate>
        <pubDate>Mon, 17 Jul 2017 16:03:43 -0700</pubDate>
        <ttl>60</ttl>


        <item>
                <title>Re-uniting Palimpsest Pages in IIIF</title>
                <description>
&lt;p&gt;#Re-uniting Palimpsest Pages in IIIF&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;In our work with the &lt;a href=&quot;http://digi.vatlib.it/&quot;&gt;Biblioteca Apostolica Vaticana&lt;/a&gt;, we have been experimenting with a particular use-case in the “digital manuscript reconstruction” space. Specifically, when a manuscript was scraped clean and then repurposed to receive new text, the original leaves were often folded and inserted into new quire structures. For a scholar working with the undertext of a palimpsest, it is extremely useful to be able to manipulate the disjunct leaves in order to virtually reconstruct the original writing surface. &lt;a href=&quot;http://iiif.io&quot;&gt;IIIF&lt;/a&gt; theoretically makes this possible, though in practice it is clear that additional tooling will be necessary to completely satisfy scholarly use-cases.&lt;/p&gt;

&lt;p&gt;We’re very fortunate to be able to collaborate with Dr. András Németh and his colleagues at the Vatican, as their use-cases will help inform development work within the IIIF sphere that will be generalizable to other areas of study. Dr. Németh has provided four examples, drawn from the Vatican collection, of digital pages that need to be manipulated in various ways in order to support study of the undertext of several palimpsests.&lt;/p&gt;

&lt;p&gt;In the four examples presented here, the issues of reconstruction and representation can be modeled in IIIF and, to greater or lesser success, viewed through existing software. I’ll start with two fairly simple examples, where two full images need to be fitted back together and rotated.&lt;/p&gt;

&lt;h3 id=&quot;example-1-vatgr103---ff-142v-and-147r&quot;&gt;Example 1: Vat.gr.103 - ff. 142v and 147r&lt;/h3&gt;

&lt;p&gt;In this instance, we need to create a single &lt;a href=&quot;http://iiif.io/api/presentation/2.1/#canvas&quot;&gt;canvas&lt;/a&gt; that includes the two surfaces (ff. 142v and 147r), and then rotate them by 90 degrees clockwise in order to see the two column undertext in its original configuration.&lt;/p&gt;

&lt;p&gt;We can address each image separately thanks to IIIF as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0298_fa_0142v.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg&quot; alt=&quot;alt text&quot; title=&quot;142v: http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0298_fa_0142v.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg&quot; /&gt;
142v: “http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0298_fa_0142v.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg”&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0308_fa_0147r.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg&quot; alt=&quot;alt text&quot; title=&quot;147r: http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0308_fa_0147r.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg&quot; /&gt;
147r: “http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0308_fa_0147r.%5B02.wl.0000%5D.jp2/full/pct:10/0/native.jpg”&lt;/p&gt;

&lt;p&gt;For demonstration purposes, I have limited the images to 10 percent of their overall size (“pct:10” in the URLs). However, we will be dealing with the full size image or, to be honest, the largest size that the Vatican will deliver. To determine the largest image size delivered from a IIIF server, we can use the image API pattern that delivers information about the image: “http://digi.vatlib.it/pub/digit/MSS_Vat.gr.103/iiif/Vat.gr.103_0298_fa_0142v.%5B02.wl.0000%5D.jp2/info.json”.&lt;/p&gt;

&lt;p&gt;The results of this request, give us:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;tiles&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;scaleFactors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;protocol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;sizes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;307&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;438&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;614&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;877&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1228&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1755&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;profile&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/level1.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;formats&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;jpg&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;qualities&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;native&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gray&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;supports&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;regionByPct&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sizeByForcedWh&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sizeByWh&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sizeAboveFull&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rotationBy90s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;mirroring&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gray&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@context&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/context.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3510&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;From this, we see that the overall size delivered for this image is 2456w x 3510h. A quick check of the other image we are working with shows dimensions of 2456w x 3508h.&lt;/p&gt;

&lt;p&gt;We know, then, that our composite canvas, which includes both images, will be 2456 x 2 = 4912w and 3510h (the larger of the two image heights). Per Dr. Németh’s instructions, this is what the composite canvas should look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/palimpsest_example1.png&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our task, then is to create a IIIF canvas that can accommodate both images. We’ll create a stand-alone &lt;a href=&quot;http://iiif.io/api/presentation/2.1/#manifest&quot;&gt;manifest&lt;/a&gt; for this example. See &lt;a href=&quot;http://dms-data.stanford.edu/data/manifests/test/test1/manifest.json&quot;&gt;here&lt;/a&gt; to examine the full manifest in detail. Ignoring the preamble, let’s look directly at the new canvas:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://dms-data.stanford.edu/data/manifests/BAV/palimp-test1/canvas/canvas-01&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sc:Canvas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Example 1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4912&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3510&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;images&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiif/MSS_Vat.gr.103/res/anno0298&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa:Annotation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;motivation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sc:painting&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiif/MSS_Vat.gr.103/res/img0298&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dctypes:Image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;image/jpeg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;service&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@context&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/context.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiifimage/MSS_Vat.gr.103/Vat.gr.103_0298_fa_0142v.%5B02.wl.0000%5D.jp2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
             &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;profile&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/level2.json&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
           &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3510&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;on&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://dms-data.stanford.edu/data/manifests/BAV/palimp-test1/canvas/canvas-01#xywh=0,0,2456,3510&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiif/MSS_Vat.gr.103/res/anno0308&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;oa:Annotation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;motivation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sc:painting&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiif/MSS_Vat.gr.103/res/img0308&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dctypes:Image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;image/jpeg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;service&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@context&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/context.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;@id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://digi.vatlib.it/iiifimage/MSS_Vat.gr.103/Vat.gr.103_0308_fa_0147r.%5B02.wl.0000%5D.jp2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
         &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;profile&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://iiif.io/api/image/2/level2.json&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3508&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
     &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;on&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://dms-data.stanford.edu/data/manifests/BAV/palimp-test1/canvas/canvas-01#xywh=2456,0,2456,3508&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;example-2-vatgr19---ff-ivv-and-1r&quot;&gt;Example 2: Vat.gr.19 - ff. IVv and 1r&lt;/h3&gt;

&lt;h3 id=&quot;example-3-vatgr984---ff-162r-and-157v&quot;&gt;Example 3: Vat.gr.984 - ff. 162r and 157v&lt;/h3&gt;

&lt;h3 id=&quot;example-4-vatgr984---ff-169av-and-169r&quot;&gt;Example 4: Vat.gr.984 - ff. 169av and 169r&lt;/h3&gt;

</description>
                <link>http://blalbrit.github.io/2017/07/07/palimpsest_pages</link>
                <guid>http://blalbrit.github.io/2017/07/07/palimpsest_pages</guid>
                <pubDate>Fri, 07 Jul 2017 00:00:00 -0700</pubDate>
        </item>

        <item>
                <title>2017-02-26 - Questions 9 (Dylan)</title>
                <description>&lt;p&gt;#Networks and Data Visualisation&lt;/p&gt;

&lt;p&gt;These readings provided some useful information for working with data, connecting that data into networks, and also visualising those networks using different kinds of graphics. We’ve already begun to do much of this in class, for example with Voyant tools. 
I think the point in Tooling Up about accountability is an important one; graphs and other visualisations present data in such a shiny, appealing way (if done correctly) that they can also be used to misdirect. This is because the veneer of the graph creates the impression that the data behind the graph is objective and collected without any bias–however, as we’ve seen, data selection and manipulation is a highly subjective process. The graph or visualisation offers another degree of separation from the data, so one must be even more aware of the fact that the original data is not necessarily infallible.&lt;/p&gt;

&lt;p&gt;I had one question about data networks– in what situation would one use a self-loop?&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/02/26/dylanquestions9</link>
                <guid>http://blalbrit.github.io/2017/02/26/dylanquestions9</guid>
                <pubDate>Sun, 26 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-15 - Questions 8 (Dylan)</title>
                <description>&lt;p&gt;##Textual Analysis Readings
The readings for this week all addressed a common topic: The applications of textual analysis, specifically stylistics.
In the Burrows reading, we get a few examples of how quantitative text analysis can help us answer questions about texts, but also raise new ones. This analysis was mostly stylistic, focusing on counts of certain words across several texts. This reminded me of Franco Moretti’s ideas about a quantitative theory of genre. However, the Burrows reading approach seemed to encourage the use of text analysis not to make sweeping claims, but rather to view texts from a different perspective and use quantitative textual analysis in tandem with interpretative work. 
This mixed approach was also emphasized in the Sinclair and Rockwell reading, in the idea of the “agile interpretive cycle:”&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“We read texts we enjoy, we then explore and study them with analytic tools and visualization interfaces, which then brings us back to rereading the texts differently.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This somewhat addresses the problem some humanists have with Franco Moretti’s ideas, namely that there’s a lack of real meaning and purpose as human interpretation takes a backseat to computer processes. We require both the speed and scale of Moretti’s distant reading, as well as the specificity and insight of close reading.&lt;/p&gt;

&lt;p&gt;That’s all easily said, but implementing it is tough. When you want to do a project quickly, how do you ensure the data is up to the standards of close reading?&lt;br /&gt;
In Linguistics 1 last quarter, we did a study of a collection of data from conversations in Palo Alto cafés. However, since our data wasn’t correctly formatted in a lot of cases, and we didn’t have time to go through and correct all of it, there were some inaccuracies that slipped through the cracks. &lt;br /&gt;
The article on Google’s NGram viewer mentioned in the Sinclair and Rockwell reading (http://searchengineland.com/when-ocr-goes-bad-googles-ngram-viewer-the-f-word-59181) also mentions some potential pitfalls. What resources/strategies exist to tackle these problems?&lt;/p&gt;

</description>
                <link>http://blalbrit.github.io/2017/02/15/dylanquestions8</link>
                <guid>http://blalbrit.github.io/2017/02/15/dylanquestions8</guid>
                <pubDate>Wed, 15 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-13 - Questions 7 (Dylan)</title>
                <description>&lt;p&gt;##MSS Codex 0877 and Linked Data
In the 2014 report by Smith et al., I noticed in the transcription it mentioned a [hole in the substrate between a and q prior to the moment of writing]. This led me to reflect on all the minute details necessitated by a study like this, and how difficult it is to translate those details into machine-readable data. My question then was: How do we mark up (and standardize marking up) a detail like  holes in the substrate prior to writing?&lt;/p&gt;

&lt;p&gt;After the Linked Data reading, my question was: Do we need to? This has more to do with what we’re trying to accomplish in the field. I think Elaine has had some very apt questions about the purpose of digitial humanities; I get very excited about the prospect of interoperability and cross-platform tools, but there’s also the importance of what we do with those tools. The reading also seemed to suggest that XML provided &lt;em&gt;too much&lt;/em&gt; flexibility, to the point where it’d be difficult to standardize (a balance we’ve discussed before). 
A question from this reading that interested me was:&lt;/p&gt;

&lt;p&gt;“How can a system process information without regard to its meaning and simultaneously generate meaning in the experience of its users?”&lt;/p&gt;

&lt;p&gt;This is essentially what we’re trying to do; we’re trying to have all the advantages of machine-readable data (quick processing and analysis, quantifiable, flexible, accessible) along with the advantages of traditional humanities (“control, provenance, transparency, reproducibility, and all the other elements of good research.”) Is this a correct assessment? (Have I been repeating this sentiment?)&lt;/p&gt;

&lt;p&gt;Also, could we go over the difference between Linked Data and Semantic Mapping?&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/02/13/dylanquestions7</link>
                <guid>http://blalbrit.github.io/2017/02/13/dylanquestions7</guid>
                <pubDate>Mon, 13 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-13 - Exercise 3 (Dylan)</title>
                <description>&lt;p&gt;##Exercise 3: T-PEN and Dragmaticon 
#####(Alternate title: T-PEN is mightier than T-SWORD)&lt;/p&gt;

&lt;p&gt;I found T-PEN very easy to use in this application, for most of the text. The text that was not in a straight line posed an obvious problem, so I did not transcribe it. There were many abbreviations I didn’t know (most of them), and still many I’m unsure about, but the built-in abbreviation glossary helped immensely. There were many instances where text would be ambiguous (minims and the like) so I would go with what seemed more likely. It seems like there’s some degree of intuition involved in transcribing. That said, one must avoid the impulse to assume what the text must say; the importance is often in the differences and abnormalities.&lt;/p&gt;

&lt;p&gt;My transcription of images 2 and 5 follows.&lt;/p&gt;

&lt;p&gt;Image 2:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Taurum aut[em] et [vir]ginem et capricornum&lt;br /&gt;
frigidos esse et siccos. Geminos libram et&lt;br /&gt;
aquarium calidos et humidos. Cancrum, Sco[r]&lt;br /&gt;
pionem, pisces frigidos et humidos. Quedam&lt;br /&gt;
e[s]t masculini sexus, quedam feminim esse di&lt;br /&gt;
xerunt! que om[n]ia falsa et nugatoria pre&lt;br /&gt;
terire, dignum duximus. Dux expecto quid de&lt;br /&gt;
alio ?sibili circulo sentias. pbus Alter de visibi&lt;br /&gt;
-lius a grecus[?] galaxias, idem Lacteus circulus&lt;br /&gt;
nu[n]cupatur Galac e[n]im lac, nos e[s]enius Iste iux.&lt;br /&gt;
Septentrione[m] a parte orientis i[n]cipit i[n] obliquu[m?]&lt;br /&gt;
ad cancru[m] ascendens p[ro] medium tor[r?]ide ad cap[i]u[m?]&lt;br /&gt;
descendit: exquo p[ro] i[n]?e?ius emisp[r]ium ad principiu[m]&lt;br /&gt;
fini[s] ?euertitur . Nominatur aut[em] Lactus p[ro]p[ter] nobi&lt;br /&gt;
-lem sui splendore[m]: Cuius rei causam si s[?]re de&lt;br /&gt;
-sideras ma[?]xobium legas. Nouem alii c[?]ctuli &lt;br /&gt;
sunt i[n]uisibiles: nihil e[n]im aliud sunt q[uae] linee&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Image 5:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“quas de statu et retrogradatione planetarum&lt;br /&gt;
Legi. Nullam de hu[iu]s [effano ut] affirmo, ?oel&lt;br /&gt;
danno. Secundum platone[m], quartus planeta[rum]&lt;br /&gt;
est venus stella videlicit i[n]qualitatibus tempe&lt;br /&gt;
-rata: Inde beni?nula dicitur: modo i[n]duodecim&lt;br /&gt;
fere mensibus, modo i[n] undecim zodiatu[m] circu&lt;br /&gt;
-iens. dicitur cu[m] marte adulterata quid superi&lt;br /&gt;
-ori parte sui tituli[?] existens facta marti bi&lt;br /&gt;
-tina minus [?] beniuola. Quintus e[x] metu[a?]&lt;br /&gt;
fere in anno cursum suum p[ro]ficiens: cumquo be&lt;br /&gt;
-nus adulterata dicitur: cum e[n]im est i[n] inferiori&lt;br /&gt;
parte sui circuli illius se miscet qualitatibus.&lt;br /&gt;
[Dux] Quare dicis venerem quartum, mercurium&lt;br /&gt;
qiuntum, secundum platonem: dixere ne aliter aly[?]&lt;br /&gt;
[p.bus] Egipty, quos sequtus est plato dixere sole[] statu&lt;br /&gt;
-tum esse post Lunam: supra upm[] mercuriu[?] esse!&lt;br /&gt;
Supra ipsum Venerem: Caldei quos sequ[i]tus e[x] tulli[?]&lt;br /&gt;
volunt post Luna[] esse mercurium: supra quem&lt;br /&gt;
est Venus! supra quam sol. Dux Mitu[?] est, tu[]&lt;br /&gt;
de ordine ceterorum no[] dubitarint, qua?te de ordi&lt;br /&gt;
-ne istatu[?] trium diuctsa sense[]ut. pbus Cirtulis&lt;br /&gt;
isto[?] huc [9?]tingit de quibus hoc dicAm ut appa 
-reat que sententia est potuus tenenda. de inde de&lt;br /&gt;
motu utoru[m] dicam. Cirtuli igitur veneris et mer&lt;br /&gt;
curi[i] sunt epiticule:idest supra ter[r]am existe[tes]!&lt;br /&gt;
nihil de? ea concludentes!horum circulo[rum] centru[us]&lt;br /&gt;
est i[n] sole: Sed mercurialis i[n] medio solis: ve&lt;br /&gt;
neris, supra medium :quod ut melius i[n]telligas&lt;br /&gt;
figuram e[x]pingam i[n] qua ter[r]am epuiculos veneris&lt;br /&gt;
et mercury, utrumq[ue] i[n] suo epi[u?]iculo i[n] duobus&lt;br /&gt;
Locis i[n] sum[m]o et imo depingam. Cum igit[ur?] ven[??]&lt;br /&gt;
et mercuruis i[n] sup[er]ioribus partibus suor[um] circu&lt;br /&gt;
lorum existunt [?]ere supra-solem sunt! tut [sp?]&lt;br /&gt;
e[n] mercurius propinnq[u]or soli. Sed cum sint i[n] infe&lt;br /&gt;
rioribus partibus earum circulo[rum] tunc e[n] sol&lt;br /&gt;
supra illos: venus q[ua] propinquior soli. Caldei&lt;br /&gt;
adhoc respicientes q[ua] quandoq[ue?] i[n]feriores su[n]t sole&lt;br /&gt;
tunc q[uam] liberius apparent! Sol e[n]im no[n] t[u]m oculi subiecta,&lt;br /&gt;
De Venere.&lt;br /&gt;
De me[r]curio&lt;br /&gt;
Saturnus&lt;br /&gt;
Juppiter&lt;br /&gt;
Mars&lt;br /&gt;
Venus&lt;br /&gt;
Mercury&lt;br /&gt;
SoL&lt;br /&gt;
Venus&lt;br /&gt;
Terra”&lt;/p&gt;
&lt;/blockquote&gt;
</description>
                <link>http://blalbrit.github.io/2017/02/13/dylanexercise3</link>
                <guid>http://blalbrit.github.io/2017/02/13/dylanexercise3</guid>
                <pubDate>Mon, 13 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-08 - Questions 6 (Dylan)</title>
                <description>&lt;p&gt;###DigiPal reading&lt;br /&gt;
DigiPal seems like a cool project. I’ve been thinking about automated transcription for a while (using computer recognition of paleography) and just figured it wasn’t yet possible. Stokes gives some reasons for why it isn’t–for instance, medieval manuscripts are really quite irregular. Even on formatted and more modern printed texts, OCR still requires human verification most of the time, so for medieval manuscripts it’d make all kinds of mistakes(I imagine)–hence the need for DigiPal and drawing boxes around a lot of letters. That said, will we ever reach the point where automated transcription is possible/viable?&lt;/p&gt;

&lt;p&gt;###IIIF reading&lt;br /&gt;
Fairly straightforward; IIIF arose out of a need for interopability(it’s right there on the tin), because image repositories were “walled gardens of technology, with institutions implementing similar solutions in vastly different ways with few mechanisms for easy exchange of data and little sharing of code or methodology” (1). I wondered about how progress is made. There’s a popular idea that progress requires some type of destruction–growing pains, revolution, etc.– but these types of projects seem to necessitate non-intrusive progress, where older systems can remain compatible. What different things does one have to consider when approaching progress/improvements this way?&lt;/p&gt;

&lt;p&gt;###SharedCanvas reading&lt;br /&gt;
Basically, this reading highlighted the considerations of being able to document even the oddest objects (use cases). It seems so simple, yet very important, that we have to be able to record information even about the &lt;em&gt;lack&lt;/em&gt; of a page. Also, how does one even go about transcribing a palimpsest?&lt;/p&gt;

</description>
                <link>http://blalbrit.github.io/2017/02/08/dylanquestions6</link>
                <guid>http://blalbrit.github.io/2017/02/08/dylanquestions6</guid>
                <pubDate>Wed, 08 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-04 - Questions 5 (Dylan)</title>
                <description>&lt;p&gt;##Readings for February 1 (Week 4)
###Introduction to TEI and XML
I found these introductions to be very, very gentle indeed–somewhat condescending, actually–but they definitely showed how simple markup languages really are. 
(One note about the Mueller article: many of the links to examples are broken.)
I remember how, in the Unsworth reading, he pointed out how the Web is a really sub-par implementation of hypertext. In the Introduction to TEI, Mueller expands on this, giving the example of the table element being used to jury-rig web page format. This is why HTML “always lives in sin because it constantly violates the cardinal rule of separating information from the mode of its display.”&lt;/p&gt;

&lt;p&gt;To me, this was the most interesting point of the reading. The process of learning to write and writing is inherently graphical (cf. “graphein”), and we format parts of text without even being aware of it. To separate these things seems very strange. That’s why it took me a little while to realize that markup itself doesn’t necessarily dictate format–all it does is mark up. That’s it. You can then apply style sheets that will give rules to display the information, based on the indications of the markup.&lt;/p&gt;

&lt;p&gt;This also relates to the Owens, Flanders, and Jannidis readings and the idea of “raw data.” Even when we mark things up, we make judgments about what parts should be demarcated into which categories. Can we ever have a data set that is independent from this subjectivity–and should we? We have TEI for some degree of standardization, at least, but as Hawkins notes:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“People sometimes find TEI more frustrating than markup languages used in other domains because it isn’t a finite standard but rather a framework that is designed to be customized for your needs, and because it often provides more than one way to do what appears to be the same thing in order to accommodate the needs of different scholars.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, even with TEI, there’s some degree of flexibility–but perhaps that’s as it should be. There are always going to be edge cases and different types of scholarship that need to be accomodated for if we want the field to be inclusive.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/02/04/dylanquestions5</link>
                <guid>http://blalbrit.github.io/2017/02/04/dylanquestions5</guid>
                <pubDate>Sat, 04 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-02-04 - Questions 3 (Dylan)</title>
                <description>&lt;p&gt;#Questions from Week 3’s Readings (Owens, Flanders &amp;amp; Jannidis) 
These readings both explored how the field of digital humanities is a delicate balance. As we make strides in bringing textual media into the digital world, we must be cautious of pitfalls along the way.&lt;/p&gt;

&lt;p&gt;###Owens: Defining Data for Humanists
Owens’ short essay suggested an interpretation of data as both human-made and computer-processable. Data sets constructed by scholars will always have a subjective quality to them; decisions must be made about what data is selected, how it’s organized, etc. However, the result is (hopefully, given proper formatting) able to be processed by a computer, and analyzed in the most strict, literal way. This dual nature is tricky. Owens mentions Franco Moretti, whose ideas about “distant reading” caused much controversy among the literary community. His detached, data-driven method flew in the face of close reading (I checked out &lt;a href=&quot;http://www.nytimes.com/2011/06/26/books/review/the-mechanic-muse-what-is-distant-reading.html?pagewanted=all&quot;&gt;this NYT article on Moretti,&lt;/a&gt; as well as &lt;a href=&quot;https://nplusonemag.com/issue-3/reviews/adventures-of-a-man-of-science/&quot;&gt;this critical article&lt;/a&gt;, although, ironically enough, I only skimmed the latter). 
The main criticism of Moretti seems to be that literature is far too complex to be quantified or organized into neat boxes (although that’s heavily simplifying it.) It’s a fair point. Perhaps it may be dangerous to believe that, just because we can analyze and quantify some aspects of some texts–such as certain word counts or character interactions–we can form vast overarching literary theories with this data.&lt;/p&gt;

&lt;p&gt;###Flanders &amp;amp; Jannidis
This reading connects well with the idea mentioned above, particularly in the final paragraph:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Obviously there is a conflict of values here: the computer-science perspective makes us look for a good general description applicable to all entities, while the humanities perspective makes us look for those features which make this entity special: models conceal when they reveal (McCarty, 2005:52). As digital humanists we feel the pull in both directions, and simple solutions would only sacrifice one side to the other.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the dual nature of digital humanities. This is also why human-computer interactions are so fascinating. Humans design things (data sets, code, models) to be read by computers. These creations have a touch of human subjectivity and individuality to them; everyone has their own coding style, everyone has their own ideas about what’s important when crafting a data set. Computers, on the other hand, don’t have any emotions at all, nor have they any need of them. This also happens to be why people are much easier to trick than computers, and why it’s not quite accurate to use computer metaphors to talk about the human brain. I agree with Flanders and Jannidis that navigating these different perspecitves will not be simple.&lt;/p&gt;

&lt;p&gt;(Although, I must say, I disagree with their formatting choices–I kept having to re-read lines due to eyeskip.)&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/02/04/dylanquestions3</link>
                <guid>http://blalbrit.github.io/2017/02/04/dylanquestions3</guid>
                <pubDate>Sat, 04 Feb 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-31 - Exercise 2 (Dylan)</title>
                <description>&lt;p&gt;##Exercise 2: Thoughts on the Dragmaticon&lt;/p&gt;

&lt;p&gt;This book intrigued me for several reasons, principally the eye-catching chimera on the first page. The illustrations are quite curious, but make sense for a textbook on science (not unlike modern textbooks.) I also wondered what science was being proposed along with these illustrations; it seemed like mostly astrology, with some astronomy and geography perhaps thrown in–these pages, then, concerned the movement of celestial bodies. The description notes that the Dragmaticon “attempts to reconcile discrepancies between church doctrine and scientific observation,” another intriguing snippet. It seems to me that church doctrine in the 1150s did view earth as a sphere (rather than flat), which we can see on page 4, 5, and 7. However, Copernicus’ heliocentric model didn’t come along until the 16th century, and it seems like the cosmology of the Dragmaticon is geocentric (cf. the page 6 illustration of the sun’s orbit around Earth). As I searched on of the pages, there was mention of a certain “galaxias idem Lacteus circulus.” They knew about the Milky Way galaxy? I suppose this isn’t completely incompatible with a geocentric view.&lt;/p&gt;

&lt;p&gt;I was very curious about William of Conches’ thoughts on the astrological signs. Although astrology is total bunk, it still appeals to a certain urge to believe that one has a deep and personal relationship with the stars and planets. On the top of page 2, it reads:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“Arietem, Leonem, et Sagiptarium calidos esse ET SICCOS.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I looked into it, and these three signs are associated with fire; they are hot and dry. Not sure why this statement has such a prominent place on the page. I suppose it had to start somewhere, as the text below the diagram continues this pattern: Taurus and Virgo and Capricorn are cold and dry (earth), Gemini, Libra, and Aquarius are hot and wet (air), and Cancer, Scorpio, and Pisces are cold and wet (water). I continued reading: Some signs are masculine, some feminine–still in keeping with modern astrology–wait, what’s this?&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;“qu[a]e om[n]ia, quasi falsi et nugatoria, praeterire
dignum duximus.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“Nugatoria” is the etymological root of a modern word (which I’m totally going to start using), “nugatory,” which means “trifling.” Apparently, all this stuff about signs being associated with elements is false and trifling? The question, then: what about astrology isn’t?&lt;/p&gt;

&lt;p&gt;Perhaps the circle diagram provides an accurate introduction to astrology? Each of the 12 astrological signs are given a 30 degree sector of the diagram, which is also filled with concentric circles. I noted that Aries, Leo, and Saggitarius (the fire signs) were equidistantly spaced, and this holds true for each of the four elements (interesting coincidence for being “trifling”). There’s a great deal of empty space in the concentric circles, with the exception of the sectors of Pisces and Aquarius. To be honest, I wasn’t able to decipher much text in these sectors besides the word “cretulus” followed by the name of a planet, and “cretulus” isn’t a word I’m aware of (or was able to find in a dictionary). &lt;br /&gt;
I was, however, able to find a similar chart from a 1549 work by Oronce Fine :
&lt;img src=&quot;http://socks-studio.com/img/blog/celestial-mechanics-09.jpg&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
(&lt;a href=&quot;http://socks-studio.com/2012/02/13/celestial-mechanics-by-oronce-fine-1549/&quot;&gt;source&lt;/a&gt;)&lt;br /&gt;
This also shows the different months associated with the signs.&lt;/p&gt;

&lt;p&gt;I suppose I haven’t yet been able to answer my question–i.e., what is the Dragmaticon trying to tell us about astrology, specifically through its graphs? However, looking at this cosmology, I can see how people have used science over the years to try to discern the order of the world; these graphs and charts are so neatly geometrical, and looking at them it’s hard not to believe that the sun doesn’t orbit the earth or that astrology is real. It also makes one question what beliefs we hold today that future generations will see as amusingly inaccurate.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/31/dylanexercise2</link>
                <guid>http://blalbrit.github.io/2017/01/31/dylanexercise2</guid>
                <pubDate>Tue, 31 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-29 - Questions 4 (Dylan)</title>
                <description>&lt;p&gt;###Questions from readings for Monday&lt;br /&gt;
The Renear reading included many familiar concepts from computer science; for instance, “macros,” (a term drawn from assembly language programming,) which are abbreviations that refer to a long string of commands. In Java, you do basically the same thing with “methods,” which allow you to designate some lines of code into a reusable command; I’ve also heard them referred to as subroutines, functions, and procedures.&lt;/p&gt;

&lt;p&gt;The need for these abbreviations is clear; you’d rather tell someone to pass you something (full method) than tell them which of their muscles to individually extend and contract (individual commands). These methods also have the advantage that, when you change the original instructions, it applies to all instances of that method/macro/subroutine, so you don’t have to change everything. Super simple stuff, but super important.&lt;/p&gt;

&lt;p&gt;I wasn’t sure what to make of SGML, as I hadn’t heard of it before. What is its function as a “meta-language?” Also, is XML strictly better than HTML (from what I’ve heard, XML is phasing out HTML at least in webpage design.)&lt;/p&gt;

&lt;p&gt;I was also glad to read more about TEI. My favorite quote was “One might say that the TEI is an agreement about how to express disagreement.” Because there are so many different ways to approach text encoding, one method of standardization might not be to say “everyone do it this way,” but to say “how do we reconcile all these different ways?”&lt;/p&gt;

&lt;p&gt;In the Kerby-Fulton reading, I immediately noticed how the Eets edition capitalized “Godd,” “Fader,” “Crist,” “Sun,” and “Loeurd” when they weren’t capitalized in the manuscript. This distinction was immediately addressed: an &lt;em&gt;edition&lt;/em&gt; optimizes for ease of readibility for the modern reader, and conforms to the style of the times, whereas a &lt;em&gt;transcription&lt;/em&gt; intends to be as faithful to the original text as possible. Both have their place, but the Arundel example in particular showcases the importance of accurate transcription.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/29/dylanquestions4</link>
                <guid>http://blalbrit.github.io/2017/01/29/dylanquestions4</guid>
                <pubDate>Sun, 29 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-25 - Questions 3 (May)</title>
                <description>&lt;h2 id=&quot;questions-from-wednesdays-readings-data-modeling&quot;&gt;Questions from Wednesday’s readings (Data Modeling)&lt;/h2&gt;

&lt;p&gt;It seemed to me that the two articles were in direct contradiction about the point that any data set is shaped by human choice and subjectivity. The Flanders and Jannidis article stated that “we can draw a difference between the data itself and its information structure,” in the sense that the former is made up of objective facts and the latter is human-made.&lt;/p&gt;

&lt;p&gt;The Owens article, on the other hand, described data as an artifact, something formed by human work, arguing that ‘raw data’ is a misnomer. “Data is not in and of itself a kind of evidence but a multifaced object which can be mobilized as evidence in support of an argument.”&lt;/p&gt;

&lt;p&gt;What might be the consequences of each view?&lt;/p&gt;

&lt;p&gt;I got the sense that Flanders and Jannidis were a bit more cheeky than Owens about humanists’ take on ‘raw data’ (e.g. “humanists commonly acknowledge that their models are social constructs”). Maybe that has something to do with the different viewpoints.&lt;/p&gt;

&lt;p&gt;Also, I would love to just briefly get a rundown on what TEI and XML are, because they keep coming up and I only have a vague understanding of what their purposes are and what they do.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/25/mayquestions3</link>
                <guid>http://blalbrit.github.io/2017/01/25/mayquestions3</guid>
                <pubDate>Wed, 25 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-23 - Questions 2 (May)</title>
                <description>&lt;p&gt;First of all, I loved learning about ‘syntactical glosses’ in Clemens and Graham - basically that scribes would mark up Latin texts with word order help for people who were learning Latin. I still do that! (In other words, Scribes: They’re Just Like Us!) It seems like Dylan and I both relate to this.&lt;/p&gt;

&lt;p&gt;As far as I understood what Roberto Busa was writing about (which was admittedly not very far), it seemed like his model of the progress of the hermeneutic/interpretative current of textual informatics goes in reverse of what I would expect, in that the end goal is something like interoperability. For some reason, when I think of simple, standard tools, I imagine that they would start that way — e.g. in the beginning there was Mirador, and then lots of repositories used Mirador for their own purposes, etc. But actually, according to Busa, the &lt;em&gt;end&lt;/em&gt; goal is that simple standard tool: “It would be a sort of universal language, in binary alphabet, ‘antiBabel’, still in virtual reality … In input, therefore, everybody could use their own native disciplined language and have the desired translations in output.”&lt;/p&gt;

&lt;p&gt;To be honest, I’m not sure I completely understand what he was proposing, but the fact that his posthumous end goal is something universal and interoperable sort of gives me hope for the current (somewhat scattered) state of manuscript repositories and digital tools.&lt;/p&gt;

&lt;p&gt;In light of the Digital Mappaemundi project, too, it seems like the model is starting from raw information or materials, developing a tool to work with those, and &lt;em&gt;then&lt;/em&gt; making that tool interoperable. It seems to me that the benefit of that is allowing us to check our own assumptions about the material — if you start with an idea of what you want to get out of a type of material (e.g. maps), you might be misrepresenting that material (e.g. by assuming that medieval maps had the same purpose - portraying a geographical reality - as something like Google maps today).&lt;/p&gt;

&lt;p&gt;In the words of the Mappaemundi reading:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For digital projects focused on historical material to be fully realized, they must also grapple with material on its own terms, and acknowledge its particular concerns. Medieval maps are far more similar to literature or art in that they are representations that are not necessarily grounded in the specifics of our reality. They were not designed to correspond point for point with the globe—precision of distance and detail, a foundation of modern cartography, was irrelevant. Rather, medieval maps were aimed at helping their largely monastic audience understand their place in the world.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What do you all think of this model: &lt;strong&gt;materials —&amp;gt; tools —&amp;gt; simplicity and interoperability&lt;/strong&gt;? Am I just stating the obvious?&lt;/p&gt;

</description>
                <link>http://blalbrit.github.io/2017/01/23/mayquestions2</link>
                <guid>http://blalbrit.github.io/2017/01/23/mayquestions2</guid>
                <pubDate>Mon, 23 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-23 - Questions 2 (Dylan)</title>
                <description>&lt;p&gt;##Week 3 Discussion Thoughts &amp;amp; Questions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First of all, I just wanted to say, I thought this quote from the Busa reading was fantastic:
    &lt;blockquote&gt;
      &lt;p&gt;“Speaking must thus be taken seriously; it is sacred, as is every human person. We are far from having exhausted the precept inscribed on Apollo’s temple at Delphi, “Know thyself.” It seems, therefore, that the problem must be attacked: in its totality – with comprehensive, i.e., global, research; collectively – by exploiting informatics with its enormous intrinsic possibilities, and not by rushing, just to save a few hours, into doing the same things which had been done before, more or less in the same way as they were done before.”&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In “Developing Digital Mappaemundi,” I thought it was great how they implemented Unsworth’s work on scholarly primitives into their design. This thing looks great! Only problem is…where is it? How do I use it? I tried to find the actual tool and came across a bunch of broken links–nothing beside remains. Has it since been supplanted by a more useful tool? Is there a standard tool people are using today for map annotation, analysis, etc., and will it too be replaced?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;I really like the fact that mistakes, too, are recorded in manuscripts, and we can find evidence of them in the dots under certain phrases, or letters indicating corrected word order,etc. It gives a very human quality to the text, which you don’t get with the ability to just “backspace” everything.&lt;/li&gt;
  &lt;li&gt;We’ve also seen how true it is that many historians (et al.) are more interested in the margins than the text itself.&lt;/li&gt;
  &lt;li&gt;I wish I could have syntactical glossing for any written Latin. It’ be very useful.&lt;/li&gt;
  &lt;li&gt;It seems like the meanings of the marks &lt;em&gt;hd&lt;/em&gt; and &lt;em&gt;hs&lt;/em&gt; changed meaning over time; from &lt;em&gt;hic deorsum&lt;/em&gt; and &lt;em&gt;hic sursum&lt;/em&gt; to &lt;em&gt;hic deficit&lt;/em&gt; and &lt;em&gt;hic supple&lt;/em&gt;. This reminds me of what I believe is one of the main questions of this course and the field of digital humanities: How do we establish standards?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/23/dylanquestions2</link>
                <guid>http://blalbrit.github.io/2017/01/23/dylanquestions2</guid>
                <pubDate>Mon, 23 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-18 - Questions 1 (Dylan)</title>
                <description>&lt;p&gt;###Discussion Questions / Thoughts
Some thoughts on the readings:
Can any online resource really be said to be “future-proof?” How can we tell what standards will survive, and which won’t?&lt;/p&gt;

&lt;p&gt;I was absolutely blown away by Deborah Parker’s project on Dante’s Inferno that Unsworth’s article mentioned, considering this was 2000. As an avid video game player, I’m always excited by projects that use visualization technology, and I was also impressed that it was backwards and forwards-compatible with other standards. I was curious if there’s been any inroads on applying VR technology to humanities studies (for example, visualizing Dante’s inferno similar to the Parker project, or even depicting how ancient cities may have looked)?&lt;/p&gt;

&lt;p&gt;Finally, some unrelated material:&lt;br /&gt;
On the collapse of standards, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Year_2038_problem&quot;&gt;2038 Problem&lt;/a&gt; refers to the inability of 32-bit systems to store a time after January 17th, 2038 on their system clocks. This could potentially cause a nuclear catastrophe. Similar to my first question–how do we make things future-proof?&lt;/p&gt;

&lt;p&gt;On a less catastrophic note, check out &lt;a href=&quot;http://the-toast.net/2016/04/14/two-monks-invent-art/&quot;&gt;two monks&lt;/a&gt;, from The Toast.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/18/dylanquestions1</link>
                <guid>http://blalbrit.github.io/2017/01/18/dylanquestions1</guid>
                <pubDate>Wed, 18 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-17 - Questions 1 (May)</title>
                <description>&lt;p&gt;##Questions from Week 2 Readings&lt;/p&gt;

&lt;p&gt;The main frustration we talked about in class last week, and a theme that came up throughout the O’Donnell reading, was standardization. The narrative of his article was that, in general, online tools and editions (mostly editions) have become increasingly standardized, and these standard expectations have been a good thing. Digital editions are easier to work with, because they had to be more standardized to be usable to more viewers with the rise of the internet.&lt;/p&gt;

&lt;p&gt;But manuscript repositories are still so scattered, with apparently little consistency between them as to what information is provided about texts and objects. &lt;strong&gt;Are repositories stuck in a pre-internet age of vastly varying standards and tightly controlled usage for a specific audience?&lt;/strong&gt; And wouldn’t simplicity and adherence to strict standards allow online resources to be even more democratic (since simple sites are easier to use with poor internet access)?&lt;/p&gt;

&lt;p&gt;That leads me to my second question: in the interim between O’Donnell and Unsworth, and us today, have there been developments in crowdsourcing repositories or editions? Both seemed to value the potential of online networks of contributors, and O’Donnell praised Wikipedia’s functionality and reach. But both also seemed to see the hurdles between themselves and successfully crowdsourced tools/editions as very daunting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Are we any closer to a repository, edition, or tool with crowd-source-ability?&lt;/strong&gt; It seems to me that this might help accomplish the immense task of providing standard information on the zillions of digitized MSS out there.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/17/mayquestions1</link>
                <guid>http://blalbrit.github.io/2017/01/17/mayquestions1</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-17 - Exercise 1 (May)</title>
                <description>&lt;p&gt;##Exercise 1&lt;/p&gt;

&lt;p&gt;For Exercise 1, I chose to critique [Digital Bodleian] (http://digital.bodleian.ox.ac.uk/) because I studied abroad in Oxford last spring and I’m feeling nostalgic, and [Gallica] (http://gallica.bnf.fr/), because I’d like to find manuscripts of Venantius Fortunatus’ poetry for a research project, and I would expect to find them in France. I’m going to just browse on the Bodleian, and then do a more targeted search on Gallica.&lt;/p&gt;

&lt;p&gt;###Digital Bodleian&lt;/p&gt;

&lt;p&gt;Digital Bodleian has a beautiful interface, and it’s immediately clear how to search for items. Before searching, though, I want to browse - below the search bar, they have several collections that seem easy to look through. When I hover over the icon of each collection, a description pops up. I’ll look at Masterpieces of the Non-Western Book.&lt;/p&gt;

&lt;p&gt;Clicking on the link took me to a different webpage, still apparently connected to the Bodleian, but in a completely different format. Looking through, if I land on a certain object in the “manuscripts” subsection, I find plenty of information about the object: shelfmark (MS. Ind. Inst. Sansk. 72), place and date of origin, incipit, size, et cetera. For each piece of information, the user can search by that information; so for instance, I can search for all other MSS with “Devanagari” in the “Script” section of the metadata.&lt;/p&gt;

&lt;p&gt;Other collections, like “Christ Church, Oxford,” link to a page within the Digital Bodleian repository and have similarly thorough information in the sidebar, without the same click-to-search option. I prefer to stay in the Digital Bodleian site over having that ease of searchability.&lt;/p&gt;

&lt;p&gt;###Gallica&lt;/p&gt;

&lt;p&gt;Gallica, the repository of the National Library of France, also has an elegant homepage. It’s too bad that the English version &lt;em&gt;n’est pas encore disponible&lt;/em&gt;. But because I know that my target author is called “Venance Fortunat” in French, I’ll search that.&lt;/p&gt;

&lt;p&gt;When I search for “Venance Fortunat,” he comes up as a “suggested author.” Two documents are associated with him, one a published and edited collection of his poetry, the other a manuscript from about the 13th century (from what I can tell). Clicking on the link to the manuscript takes me to the website of the Bibliothèque Municipal de Dijon, which does not have much more information than Gallica had - just the shelf mark and a thumbnail (which links to a high resolution scan).&lt;/p&gt;

&lt;p&gt;If I want to expand my search, I’ll try ignoring the “suggested author” and searching “Fortunat.” The immediate results are quite a few scholarly works from the last two centuries. Narrowing to manuscripts, I have a bit more luck; I’ll order them by date from early to late.&lt;/p&gt;

&lt;p&gt;Once I click on a shelf mark, the metadata disappears and I have just an image — on Digital Bodleian, I liked that the metadata stayed on a sidebar onscreen. Here, I have to go back out to my search results and click “detailed information.” I’m able to find some helpful information about dating and contents, but the size of the object, the number of pages, and really any information about the object’s materiality, is lacking.&lt;/p&gt;

&lt;p&gt;For other manuscripts, I had more luck with finding information about their physical characteristics.&lt;/p&gt;

&lt;p&gt;###General observations&lt;/p&gt;

&lt;p&gt;Digital Bodleian strikes me as the more standardized repository when it comes to providing basic necessary information about objects. After searching around in Gallica, I found that upon returning to the Bodleian I was almost relieved. I had been clicking back and forth way more than seemed necessary to find any data about objects in Gallica.&lt;/p&gt;

&lt;p&gt;I found that the more types of objects or texts the repository had, the more difficult the site was to use - especially when searching for a specific kind of object. For browsing, on the other hand, I found myself down in rabbit holes of the history of French music players and drawings of “Burmese Life and Devotion.”&lt;/p&gt;

&lt;p&gt;The flashy initial interface, and the vast array of materials in each repository, are impressive to someone stumbling around the internet for fun or perhaps seeking to answer broad historical questions. But for the targeted researcher (unless &lt;em&gt;this&lt;/em&gt; researcher is particularly incompetent), the range of dates and categories was somewhat daunting, in both cases.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/17/maypost1</link>
                <guid>http://blalbrit.github.io/2017/01/17/maypost1</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-17 - Blog Post Template</title>
                <description>&lt;h1 id=&quot;example-post&quot;&gt;Example Post&lt;/h1&gt;

&lt;p&gt;Text here.&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/17/templatepost</link>
                <guid>http://blalbrit.github.io/2017/01/17/templatepost</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-17 - Questions 1 (Lynn)</title>
                <description>&lt;p&gt;John Unsworth “Scholarly Primitives: what methods do humanities researchers have in common, and how might our tools reflect this?”
http://www.people.virginia.edu/~jmu2m/Kings.5-00/primitives.html
 (Ben, re discovering: was your citation link purposfully not to the article itself?)&lt;/p&gt;

&lt;p&gt;Discovering — reliance on description, categorization .. full text left to the power of google-like search of web contentas expressed in HTML.  No reliable access to image content expressing text unless (often crudely) OCRs and/or turned into a full-test PDF at some cost. Poor lexical search power across “what’s out there”.  Yet we see this successfully achieved in niches, e.g., song lyrics, perhaps because it is current /owned/cared-for content, ready to be promoted and appreciated by a wide fan base, strong commercial backing.&lt;/p&gt;

&lt;p&gt;Annotating can be both a tool for illuminating and understanding but can also be a great too for expanding the discoverability of resources. The power of crowd sourcing …&lt;/p&gt;

&lt;p&gt;With example of “annotation and comparison: biologists do it too”, would be interesting to ponder lists of equivalences across disciplines, e.g., Selecting vs Sampling) and Comparing&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;manuscripts and maps : region of interest&lt;/li&gt;
  &lt;li&gt;music : passages, movements — instrumental lines? comparing performances?&lt;/li&gt;
  &lt;li&gt;audio/video : scenes, tracks, snippets&lt;/li&gt;
  &lt;li&gt;scientific data : isolating groups by common factos, comparing against varied other criteria (e.g., same ethnicity, different decades)?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Representing:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;canonical primitives, like geo-rectification and overlays, word clouds, timelines, bar/line/pie charts&lt;/li&gt;
  &lt;li&gt;vs uniquely revealing/artistic, like Dante visualization?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Apropos second reading “Disciplinary Impact and Technological Obsolescence in Digital Medieval Studies1”, primitives suggest enduring/evolving technolgy for representation, while specialized visualizations might suffer.  How will the Dante 360-degree canto/line/word spiral be viewed in tomorrow’s browser or game console?&lt;/p&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/17/LynnPost2</link>
                <guid>http://blalbrit.github.io/2017/01/17/LynnPost2</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>2017-01-17 - Exercise 1 (Lynn)</title>
                <description>&lt;h2 id=&quot;exercise-1&quot;&gt;Exercise 1&lt;/h2&gt;

&lt;h4 id=&quot;retail-manuscript-sites&quot;&gt;Retail manuscript sites&lt;/h4&gt;

&lt;p&gt;Charles Edwin Puckett — https://www.cepuckett.com&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;leaves only, richly described.&lt;/li&gt;
  &lt;li&gt;Medium sized images only, poor treatment of multiple images (overlapping buyers illustration only)&lt;/li&gt;
  &lt;li&gt;Philip J. Pirages — https://www.pirages.com&lt;/li&gt;
  &lt;li&gt;mixture of manuscript leaves, few books.&lt;/li&gt;
  &lt;li&gt;rich set of descriptive categories for browsing, but they were inoperative&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Griffons Medieval Manuscripts — http://www.griffons.com/&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Affordable Renaissance Art…”&lt;/li&gt;
  &lt;li&gt;Poor categorization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sanderus Antiquariat — https://www.sanderusmaps.com&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Individual leaves and cuttings (!)&lt;/li&gt;
  &lt;li&gt;A lot of commercial apparatus … cart, new additions, prices&lt;/li&gt;
  &lt;li&gt;Has large scale scans&lt;/li&gt;
  &lt;li&gt;Medieval Manuscript is just one “catalog”; entire inventory on three pages&lt;/li&gt;
  &lt;li&gt;MINIATURE&lt;/li&gt;
  &lt;li&gt;Z ANTIPHONARY LEAF&lt;/li&gt;
  &lt;li&gt;TEXT LEAVE&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;digital-manuscripts&quot;&gt;Digital Manuscripts&lt;/h4&gt;

&lt;p&gt;Monastic Manuscript Project — http://www.earlymedievalmonasticism.org/&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Grant funded project at Syracuse University&lt;/li&gt;
  &lt;li&gt;Gathers information about projects and site related to the digization and study of manuscripts&lt;/li&gt;
  &lt;li&gt;Hundreds of links show how scattered often just a very few resources are&lt;/li&gt;
  &lt;li&gt;Nothing there to work on discovery across the aggregated resources or to categorize/describe contents&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Clairvaux Virtual LIbrary — https://www.bibliotheque-virtuelle-clairvaux.com/&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“All indexed manuscript” but no mention of how many&lt;/li&gt;
  &lt;li&gt;Advanced search, no simple keywords, no auto-complete&lt;/li&gt;
  &lt;li&gt;Facets appear only for search results&lt;/li&gt;
  &lt;li&gt;Viewer for full pan/zoom: Mediatheque, Grand Troyes (coalition of libraries/archives)&lt;/li&gt;
&lt;/ul&gt;
</description>
                <link>http://blalbrit.github.io/2017/01/17/LynnPost1</link>
                <guid>http://blalbrit.github.io/2017/01/17/LynnPost1</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>

        <item>
                <title>DLCL122-2017 - Dylan's Blog Post, Exercise 1</title>
                <description>&lt;h1 id=&quot;dylans-blog-post-1&quot;&gt;Dylan’s Blog Post #1&lt;/h1&gt;
&lt;p&gt;##Exercise 1&lt;/p&gt;

&lt;p&gt;For this exercise, I chose to look at the &lt;a href=&quot;http://cudl.lib.cam.ac.uk/&quot;&gt;Cambridge University Digital Library&lt;/a&gt;&lt;br /&gt;
as well as the The British Library’s &lt;a href=&quot;http://www.bl.uk/manuscripts/&quot;&gt;repository of manuscripts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;###Finding the Manuscripts&lt;br /&gt;
Firstly, and perhaps most importantly: How easy is it to find each library’s collection of medieval manuscripts? I located both sites via &lt;a href=&quot;http://guides.nyu.edu/c.php?g=276597&amp;amp;p=1844931&quot;&gt;this guide&lt;/a&gt;, but what if I were approaching the websites from the libraries’ landing pages?&lt;br /&gt;
####Cambridge
The user interface for this page was very friendly, presenting the user with some simple and avanced search tools as well as categorized collections. However, this project was only started in 2010, and the library has about two millennia’s worth of content to digitize; thus, the collection is somewhat lacking.&lt;br /&gt;
The categorization system is at points too narrow, and at others too broad. One collection offers “Music,” another “Christian Works,” while yet another is solely dedicated to “notebooks kept by the soldier-poet Siegfried Sassoon.” In my opinion, they need to re-think this system. For instance, there’s no one category for medieval manuscripts. &lt;br /&gt;
Ultimately, I was able to find many in the “Christian Works” section, as well as the Royal Library, and thankfully the search tool allows you to search across categories.
Assuming one finds a manuscript one’s looking for, the image viewer is actually quite good. I was particularly impressed by the metadata accompanying the text. For the Roman de la Rose, this includes collation, binding, script, foliation, incipit and explicit, and a bibliography, among other data. It seems as though they put particular care into including metadata with the text, although this has the trade-off that adding new manuscripts to the repository might be a slow process.&lt;/p&gt;

&lt;p&gt;####British Library
The digital repository is much easier to find in this case. You can access the collection of manuscripts easily from the main page. The search is straightforward, allowing the user to input a keyword as well as define a date range (very useful if you’re only interested in a specific era) and returns results with a shelf mark and thumbnail. Unlike Cambridge, the image viewer allows you to view either one page, “open-book” style, or folio. Still, it lacks the comparison functionality of Mirador to accomodate more than one text.&lt;br /&gt;
This library also lacks the in-depth metadata that Cambridge provides. However, it also has many more texts as a whole. Perhaps these are also related; it’s difficult to add a great deal of texts and also include comprehensive data on each one. This makes it more difficult to find texts, though, since you have fewer tags with which to locate a text, and you’re searching in a bigger database.&lt;/p&gt;

&lt;p&gt;##Reflections&lt;br /&gt;
Overall, I think the two repositories take two different and valid approaches; Cambridge seems to have relatively few books, placed in easily-perusable categories, with a great deal of data on most of them. The British library has more books, but with less data and therefore more difficulty in finding specific ones. It seems to me that the Cambridge database’s use of “collections” almost make the manuscripts into museum pieces rather than texts with which you’re invited to interact, whereas the British Library is more of a useful tool that provides a great deal of content easily.&lt;/p&gt;

</description>
                <link>http://blalbrit.github.io/2017/01/17/Dylan-Blog-1</link>
                <guid>http://blalbrit.github.io/2017/01/17/Dylan-Blog-1</guid>
                <pubDate>Tue, 17 Jan 2017 00:00:00 -0800</pubDate>
        </item>


</channel>
</rss>
